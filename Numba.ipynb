{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Binary Logistic Regression Speed-Up in Python with Numba"
      ],
      "metadata": {
        "id": "pG3AOKGjFtbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# running the sigmoid function\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# the classifier\n",
        "def if_above(x):\n",
        "  for i in range(len(x)):\n",
        "    if x[i] >= 0.5:\n",
        "      x[i] = 1\n",
        "    else:\n",
        "      x[i] = 0\n",
        "  return x\n",
        "\n",
        "# applying logistic regression\n",
        "def calculating_logistic_regression(X_train, weights, bias):\n",
        "  z = np.dot(X_train, weights) + bias\n",
        "  y_pred = sigmoid(z)\n",
        "  return if_above(y_pred)\n",
        "\n",
        "# comparing ground truth y_true to your predictions y_pred\n",
        "def compute_loss(y_train, y_pred):\n",
        "  epsilon = 1e-9\n",
        "  m = len(y_train)\n",
        "  loss = 0.0\n",
        "  y_train = y_train.reshape(-1) # fixing up data\n",
        "  y_pred = y_pred.reshape(-1)\n",
        "\n",
        "  for i in prange(m):\n",
        "      loss += y_train[i] * math.log(y_pred[i] + epsilon) + (1 - y_train[i]) * math.log(1 - y_pred[i] + epsilon)\n",
        "  # adding epsilon so not exactly 0 and 1\n",
        "\n",
        "  return -loss / m   # mean of loss\n",
        "\n",
        "# calculate gradient to use to update the model parameters\n",
        "def gradient(y_train, y_pred, X_train):\n",
        "  m = len(y_train)\n",
        "  difference = y_pred -  y_train.reshape(-1)\n",
        "  weight_update = np.dot(X_train.T, difference) / m\n",
        "  bias_update = np.sum(difference) / m\n",
        "  return weight_update, bias_update\n",
        "\n",
        "# updating weights and bias with values\n",
        "def update_parameters(weights, bias, weight_update, bias_update, learning_rate = 0.1):\n",
        "  for i in range(weights.shape[0]):\n",
        "    weights[i] -= learning_rate * weight_update[i]\n",
        "  bias -= learning_rate * bias_update\n",
        "  return weights, bias\n",
        "\n",
        "# calculating weights and sigmoid calculation\n",
        "def fit(X_train, y_train, epochs=200, learning_rate = 0.1):\n",
        "  # given values\n",
        "  x_rows, x_columns = X_train.shape\n",
        "  weights = np.random.rand(x_columns) * 0.01  # initializing weights to small random values\n",
        "  bias = 0  # initializing bias to zero\n",
        "  y_train = y_train.reshape(-1, 1)  # Ensuring that y_train is of shape (m, 1)\n",
        "\n",
        "  epoch_times = [] # Time for each epochs run\n",
        "\n",
        "  # for a certain amount of epochs apply gradient\n",
        "  for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # applying logistic regression\n",
        "    y_pred = calculating_logistic_regression(X_train, weights, bias)\n",
        "\n",
        "    # updating weights and bias\n",
        "    weight_update, bias_update = gradient(y_train, y_pred, X_train)\n",
        "    weights, bias = update_parameters(weights, bias, weight_update, bias_update, learning_rate)\n",
        "\n",
        "    # calculating loss\n",
        "    loss = compute_loss(y_train, y_pred)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "    epoch_times.append(epoch_time)\n",
        "\n",
        "    print(\"Epoch: {} Loss: {} Time: {:.4f} seconds\".format(epoch, loss, epoch_time))\n",
        "\n",
        "  avg_epoch_time = np.mean(epoch_times[1:]) # except first cause getting set up\n",
        "  print(\"Average time per epoch: {:.4f} seconds\".format(avg_epoch_time))\n",
        "\n",
        "  return weights, bias\n",
        "\n",
        "# predicting with X_test\n",
        "def predict(X_test, weights, bias):\n",
        "  return calculating_logistic_regression(X_test, weights, bias)\n",
        "\n",
        "# load the breast cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# split the train and test dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "\n",
        "# now my part comes in will replace the previous LogisticRegression with my own\n",
        "weights, bias = fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = predict(X_test, weights, bias)\n",
        "\n",
        "# Compaaring with previous y_test\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression model accuracy (in %):\", acc * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVb_DfYBfi4L",
        "outputId": "912fb4a5-2e4f-40c0-ff0a-732feeed1f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 7.8793955813400425 Time: 0.0130 seconds\n",
            "Epoch: 1 Loss: 12.843870254606308 Time: 0.0082 seconds\n",
            "Epoch: 2 Loss: 12.843870254606308 Time: 0.0048 seconds\n",
            "Epoch: 3 Loss: 7.8793955813400425 Time: 0.0091 seconds\n",
            "Epoch: 4 Loss: 12.843870254606308 Time: 0.0054 seconds\n",
            "Epoch: 5 Loss: 7.8793955813400425 Time: 0.0053 seconds\n",
            "Epoch: 6 Loss: 12.843870254606308 Time: 0.0053 seconds\n",
            "Epoch: 7 Loss: 12.843870254606308 Time: 0.0053 seconds\n",
            "Epoch: 8 Loss: 7.8793955813400425 Time: 0.0052 seconds\n",
            "Epoch: 9 Loss: 12.843870254606308 Time: 0.0053 seconds\n",
            "Epoch: 10 Loss: 7.8793955813400425 Time: 0.0057 seconds\n",
            "Epoch: 11 Loss: 12.843870254606308 Time: 0.0053 seconds\n",
            "Epoch: 12 Loss: 11.24977288245658 Time: 0.0052 seconds\n",
            "Epoch: 13 Loss: 7.8793955813400425 Time: 0.0052 seconds\n",
            "Epoch: 14 Loss: 12.843870254606308 Time: 0.0052 seconds\n",
            "Epoch: 15 Loss: 7.8793955813400425 Time: 0.0051 seconds\n",
            "Epoch: 16 Loss: 12.843870254606308 Time: 0.0052 seconds\n",
            "Epoch: 17 Loss: 2.2317363200096243 Time: 0.0052 seconds\n",
            "Epoch: 18 Loss: 12.843870254606308 Time: 0.0051 seconds\n",
            "Epoch: 19 Loss: 7.8793955813400425 Time: 0.0095 seconds\n",
            "Epoch: 20 Loss: 12.843870254606308 Time: 0.0056 seconds\n",
            "Epoch: 21 Loss: 7.833849942135766 Time: 0.0054 seconds\n",
            "Epoch: 22 Loss: 12.843870254606308 Time: 0.0052 seconds\n",
            "Epoch: 23 Loss: 9.564584231898294 Time: 0.0061 seconds\n",
            "Epoch: 24 Loss: 7.8793955813400425 Time: 0.0078 seconds\n",
            "Epoch: 25 Loss: 12.843870254606308 Time: 0.0067 seconds\n",
            "Epoch: 26 Loss: 7.651667385318656 Time: 0.0054 seconds\n",
            "Epoch: 27 Loss: 12.843870254606308 Time: 0.0066 seconds\n",
            "Epoch: 28 Loss: 4.873383393857736 Time: 0.0076 seconds\n",
            "Epoch: 29 Loss: 7.287302271684437 Time: 0.0082 seconds\n",
            "Epoch: 30 Loss: 12.843870254606308 Time: 0.0076 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-124-322b08496249>:13: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 31 Loss: 2.2772819592139024 Time: 0.0076 seconds\n",
            "Epoch: 32 Loss: 5.101111589879124 Time: 0.0179 seconds\n",
            "Epoch: 33 Loss: 7.287302271684437 Time: 0.0089 seconds\n",
            "Epoch: 34 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 35 Loss: 2.004008123988235 Time: 0.0074 seconds\n",
            "Epoch: 36 Loss: 9.883403706328238 Time: 0.0074 seconds\n",
            "Epoch: 37 Loss: 7.8793955813400425 Time: 0.0075 seconds\n",
            "Epoch: 38 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 39 Loss: 7.196210993275882 Time: 0.0076 seconds\n",
            "Epoch: 40 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 41 Loss: 2.1406450416010685 Time: 0.0076 seconds\n",
            "Epoch: 42 Loss: 12.707233336993474 Time: 0.0112 seconds\n",
            "Epoch: 43 Loss: 7.8793955813400425 Time: 0.0106 seconds\n",
            "Epoch: 44 Loss: 12.843870254606308 Time: 0.0072 seconds\n",
            "Epoch: 45 Loss: 7.332847910888714 Time: 0.0075 seconds\n",
            "Epoch: 46 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 47 Loss: 2.8238296296652363 Time: 0.0079 seconds\n",
            "Epoch: 48 Loss: 12.843870254606308 Time: 0.0155 seconds\n",
            "Epoch: 49 Loss: 7.8793955813400425 Time: 0.0094 seconds\n",
            "Epoch: 50 Loss: 12.843870254606308 Time: 0.0086 seconds\n",
            "Epoch: 51 Loss: 6.37638948759889 Time: 0.0075 seconds\n",
            "Epoch: 52 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 53 Loss: 5.511022342717619 Time: 0.0076 seconds\n",
            "Epoch: 54 Loss: 12.843870254606308 Time: 0.0082 seconds\n",
            "Epoch: 55 Loss: 7.014028436458773 Time: 0.0169 seconds\n",
            "Epoch: 56 Loss: 12.843870254606308 Time: 0.0076 seconds\n",
            "Epoch: 57 Loss: 3.780288052955072 Time: 0.0075 seconds\n",
            "Epoch: 58 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 59 Loss: 7.74275866372721 Time: 0.0076 seconds\n",
            "Epoch: 60 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 61 Loss: 5.32883978590051 Time: 0.0074 seconds\n",
            "Epoch: 62 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 63 Loss: 7.105119714867327 Time: 0.0076 seconds\n",
            "Epoch: 64 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 65 Loss: 3.962470609772183 Time: 0.0074 seconds\n",
            "Epoch: 66 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 67 Loss: 7.651667385318656 Time: 0.0076 seconds\n",
            "Epoch: 68 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 69 Loss: 5.101111589879124 Time: 0.0074 seconds\n",
            "Epoch: 70 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 71 Loss: 7.105119714867327 Time: 0.0083 seconds\n",
            "Epoch: 72 Loss: 12.843870254606308 Time: 0.0076 seconds\n",
            "Epoch: 73 Loss: 4.46347264101924 Time: 0.0165 seconds\n",
            "Epoch: 74 Loss: 12.843870254606308 Time: 0.0075 seconds\n",
            "Epoch: 75 Loss: 7.332847910888714 Time: 0.0055 seconds\n",
            "Epoch: 76 Loss: 12.843870254606308 Time: 0.0053 seconds\n",
            "Epoch: 77 Loss: 5.101111589879124 Time: 0.0052 seconds\n",
            "Epoch: 78 Loss: 12.843870254606308 Time: 0.0055 seconds\n",
            "Epoch: 79 Loss: 7.105119714867327 Time: 0.0052 seconds\n",
            "Epoch: 80 Loss: 12.843870254606308 Time: 0.0106 seconds\n",
            "Epoch: 81 Loss: 4.64565519783635 Time: 0.0055 seconds\n",
            "Epoch: 82 Loss: 12.798324615402032 Time: 0.0052 seconds\n",
            "Epoch: 83 Loss: 7.196210993275882 Time: 0.0054 seconds\n",
            "Epoch: 84 Loss: 12.843870254606308 Time: 0.0048 seconds\n",
            "Epoch: 85 Loss: 5.055565950674846 Time: 0.0052 seconds\n",
            "Epoch: 86 Loss: 12.798324615402032 Time: 0.0054 seconds\n",
            "Epoch: 87 Loss: 7.014028436458773 Time: 0.0056 seconds\n",
            "Epoch: 88 Loss: 12.843870254606308 Time: 0.0052 seconds\n",
            "Epoch: 89 Loss: 5.055565950674846 Time: 0.0064 seconds\n",
            "Epoch: 90 Loss: 12.798324615402032 Time: 0.0053 seconds\n",
            "Epoch: 91 Loss: 6.922937158050218 Time: 0.0052 seconds\n",
            "Epoch: 92 Loss: 12.843870254606308 Time: 0.0051 seconds\n",
            "Epoch: 93 Loss: 5.4654767035133425 Time: 0.0051 seconds\n",
            "Epoch: 94 Loss: 12.798324615402032 Time: 0.0052 seconds\n",
            "Epoch: 95 Loss: 6.558572044415999 Time: 0.0063 seconds\n",
            "Epoch: 96 Loss: 12.843870254606308 Time: 0.0053 seconds\n",
            "Epoch: 97 Loss: 5.738750538739007 Time: 0.0053 seconds\n",
            "Epoch: 98 Loss: 12.798324615402032 Time: 0.0052 seconds\n",
            "Epoch: 99 Loss: 6.239752569986058 Time: 0.0051 seconds\n",
            "Epoch: 100 Loss: 12.843870254606308 Time: 0.0076 seconds\n",
            "Epoch: 101 Loss: 5.966478734760393 Time: 0.0052 seconds\n",
            "Epoch: 102 Loss: 12.798324615402032 Time: 0.0051 seconds\n",
            "Epoch: 103 Loss: 5.966478734760393 Time: 0.0051 seconds\n",
            "Epoch: 104 Loss: 12.798324615402032 Time: 0.0052 seconds\n",
            "Epoch: 105 Loss: 6.1031156523732255 Time: 0.0066 seconds\n",
            "Epoch: 106 Loss: 12.798324615402032 Time: 0.0059 seconds\n",
            "Epoch: 107 Loss: 5.966478734760393 Time: 0.0103 seconds\n",
            "Epoch: 108 Loss: 12.752778976197753 Time: 0.0053 seconds\n",
            "Epoch: 109 Loss: 5.966478734760393 Time: 0.0054 seconds\n",
            "Epoch: 110 Loss: 12.707233336993474 Time: 0.0157 seconds\n",
            "Epoch: 111 Loss: 5.966478734760393 Time: 0.0056 seconds\n",
            "Epoch: 112 Loss: 12.661687697789198 Time: 0.0050 seconds\n",
            "Epoch: 113 Loss: 5.966478734760393 Time: 0.0052 seconds\n",
            "Epoch: 114 Loss: 12.661687697789198 Time: 0.0112 seconds\n",
            "Epoch: 115 Loss: 5.966478734760393 Time: 0.0074 seconds\n",
            "Epoch: 116 Loss: 12.52505078017636 Time: 0.0058 seconds\n",
            "Epoch: 117 Loss: 5.966478734760393 Time: 0.0058 seconds\n",
            "Epoch: 118 Loss: 12.479505140972082 Time: 0.0054 seconds\n",
            "Epoch: 119 Loss: 5.966478734760393 Time: 0.0052 seconds\n",
            "Epoch: 120 Loss: 12.433959501767802 Time: 0.0053 seconds\n",
            "Epoch: 121 Loss: 5.966478734760393 Time: 0.0056 seconds\n",
            "Epoch: 122 Loss: 12.433959501767802 Time: 0.0053 seconds\n",
            "Epoch: 123 Loss: 5.966478734760393 Time: 0.0052 seconds\n",
            "Epoch: 124 Loss: 12.388413862563523 Time: 0.0051 seconds\n",
            "Epoch: 125 Loss: 5.829841817147561 Time: 0.0051 seconds\n",
            "Epoch: 126 Loss: 12.0240487489293 Time: 0.0052 seconds\n",
            "Epoch: 127 Loss: 5.829841817147561 Time: 0.0051 seconds\n",
            "Epoch: 128 Loss: 11.932957470520742 Time: 0.0051 seconds\n",
            "Epoch: 129 Loss: 5.7842961779432835 Time: 0.0051 seconds\n",
            "Epoch: 130 Loss: 11.93295747052074 Time: 0.0051 seconds\n",
            "Epoch: 131 Loss: 5.829841817147561 Time: 0.0052 seconds\n",
            "Epoch: 132 Loss: 11.887411831316463 Time: 0.0051 seconds\n",
            "Epoch: 133 Loss: 5.647659260330451 Time: 0.0062 seconds\n",
            "Epoch: 134 Loss: 11.38640980006941 Time: 0.0052 seconds\n",
            "Epoch: 135 Loss: 5.738750538739007 Time: 0.0051 seconds\n",
            "Epoch: 136 Loss: 11.568592356886525 Time: 0.0051 seconds\n",
            "Epoch: 137 Loss: 5.647659260330451 Time: 0.0052 seconds\n",
            "Epoch: 138 Loss: 11.158681604048022 Time: 0.0051 seconds\n",
            "Epoch: 139 Loss: 5.647659260330451 Time: 0.0051 seconds\n",
            "Epoch: 140 Loss: 11.158681604048022 Time: 0.0052 seconds\n",
            "Epoch: 141 Loss: 5.647659260330451 Time: 0.0051 seconds\n",
            "Epoch: 142 Loss: 11.113135964843744 Time: 0.0063 seconds\n",
            "Epoch: 143 Loss: 5.602113621126175 Time: 0.0049 seconds\n",
            "Epoch: 144 Loss: 10.794316490413795 Time: 0.0052 seconds\n",
            "Epoch: 145 Loss: 5.374385425104788 Time: 0.0053 seconds\n",
            "Epoch: 146 Loss: 9.701221149511127 Time: 0.0083 seconds\n",
            "Epoch: 147 Loss: 4.964474672266292 Time: 0.0174 seconds\n",
            "Epoch: 148 Loss: 9.018036561446976 Time: 0.0120 seconds\n",
            "Epoch: 149 Loss: 4.691200837040627 Time: 0.0126 seconds\n",
            "Epoch: 150 Loss: 7.970486859748597 Time: 0.0054 seconds\n",
            "Epoch: 151 Loss: 4.053561888180739 Time: 0.0052 seconds\n",
            "Epoch: 152 Loss: 5.920933095556116 Time: 0.0052 seconds\n",
            "Epoch: 153 Loss: 3.0971034648909033 Time: 0.0055 seconds\n",
            "Epoch: 154 Loss: 4.873383393857736 Time: 0.0052 seconds\n",
            "Epoch: 155 Loss: 2.277281959213902 Time: 0.0051 seconds\n",
            "Epoch: 156 Loss: 3.8258336921593497 Time: 0.0052 seconds\n",
            "Epoch: 157 Loss: 2.0495537631925127 Time: 0.0051 seconds\n",
            "Epoch: 158 Loss: 3.461468578525127 Time: 0.0051 seconds\n",
            "Epoch: 159 Loss: 2.0040081239882346 Time: 0.0052 seconds\n",
            "Epoch: 160 Loss: 3.142649104095182 Time: 0.0052 seconds\n",
            "Epoch: 161 Loss: 1.9584624847839565 Time: 0.0052 seconds\n",
            "Epoch: 162 Loss: 2.8238296296652368 Time: 0.0051 seconds\n",
            "Epoch: 163 Loss: 2.140645041601068 Time: 0.0051 seconds\n",
            "Epoch: 164 Loss: 2.5050101552352917 Time: 0.0051 seconds\n",
            "Epoch: 165 Loss: 2.095099402396791 Time: 0.0073 seconds\n",
            "Epoch: 166 Loss: 2.7782839904609586 Time: 0.0055 seconds\n",
            "Epoch: 167 Loss: 2.140645041601068 Time: 0.0052 seconds\n",
            "Epoch: 168 Loss: 2.5050101552352917 Time: 0.0052 seconds\n",
            "Epoch: 169 Loss: 2.095099402396791 Time: 0.0052 seconds\n",
            "Epoch: 170 Loss: 2.8238296296652368 Time: 0.0054 seconds\n",
            "Epoch: 171 Loss: 2.0495537631925123 Time: 0.0104 seconds\n",
            "Epoch: 172 Loss: 2.8238296296652368 Time: 0.0053 seconds\n",
            "Epoch: 173 Loss: 2.0495537631925123 Time: 0.0053 seconds\n",
            "Epoch: 174 Loss: 2.8238296296652368 Time: 0.0052 seconds\n",
            "Epoch: 175 Loss: 2.0495537631925123 Time: 0.0051 seconds\n",
            "Epoch: 176 Loss: 2.8238296296652368 Time: 0.0053 seconds\n",
            "Epoch: 177 Loss: 2.0950994023967904 Time: 0.0057 seconds\n",
            "Epoch: 178 Loss: 2.6871927120524033 Time: 0.0077 seconds\n",
            "Epoch: 179 Loss: 2.186190680805346 Time: 0.0138 seconds\n",
            "Epoch: 180 Loss: 2.6871927120524033 Time: 0.0108 seconds\n",
            "Epoch: 181 Loss: 2.140645041601068 Time: 0.0053 seconds\n",
            "Epoch: 182 Loss: 2.8238296296652368 Time: 0.0052 seconds\n",
            "Epoch: 183 Loss: 2.0495537631925123 Time: 0.0098 seconds\n",
            "Epoch: 184 Loss: 2.8693752688695144 Time: 0.0053 seconds\n",
            "Epoch: 185 Loss: 2.0495537631925123 Time: 0.0053 seconds\n",
            "Epoch: 186 Loss: 2.9149209080737926 Time: 0.0056 seconds\n",
            "Epoch: 187 Loss: 2.0495537631925123 Time: 0.0053 seconds\n",
            "Epoch: 188 Loss: 2.9604665472780702 Time: 0.0054 seconds\n",
            "Epoch: 189 Loss: 2.0495537631925123 Time: 0.0052 seconds\n",
            "Epoch: 190 Loss: 2.9149209080737926 Time: 0.0053 seconds\n",
            "Epoch: 191 Loss: 2.0495537631925123 Time: 0.0054 seconds\n",
            "Epoch: 192 Loss: 2.9604665472780702 Time: 0.0053 seconds\n",
            "Epoch: 193 Loss: 2.0040081239882346 Time: 0.0053 seconds\n",
            "Epoch: 194 Loss: 2.9604665472780702 Time: 0.0126 seconds\n",
            "Epoch: 195 Loss: 2.0040081239882346 Time: 0.0053 seconds\n",
            "Epoch: 196 Loss: 2.9604665472780702 Time: 0.0052 seconds\n",
            "Epoch: 197 Loss: 2.0040081239882346 Time: 0.0052 seconds\n",
            "Epoch: 198 Loss: 2.9604665472780702 Time: 0.0052 seconds\n",
            "Epoch: 199 Loss: 2.0040081239882346 Time: 0.0052 seconds\n",
            "Average time per epoch: 0.0066 seconds\n",
            "Logistic Regression model accuracy (in %): 85.08771929824562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parallelized version\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import math\n",
        "from numba import njit, prange\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "@njit\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "@njit(parallel=True)\n",
        "def if_above(x):\n",
        "  for i in prange(len(x)):\n",
        "    if x[i] >= 0.5:\n",
        "      x[i] = 1\n",
        "    else:\n",
        "      x[i] = 0\n",
        "  return x\n",
        "\n",
        "# applying logistic regression\n",
        "@njit\n",
        "def calculating_logistic_regression(X_train, weights, bias):\n",
        "  z = np.dot(X_train, weights) + bias\n",
        "  y_pred = sigmoid(z)\n",
        "  return if_above(y_pred)\n",
        "\n",
        "# comparing ground truth y_true to your predictions y_pred\n",
        "@njit(parallel=True, fastmath = True)\n",
        "def compute_loss(y_train, y_pred):\n",
        "  epsilon = 1e-9\n",
        "  m = len(y_train)\n",
        "  loss = 0.0\n",
        "  y_train = y_train.reshape(-1) # fixing up data\n",
        "  y_pred = y_pred.reshape(-1)\n",
        "\n",
        "  for i in prange(m):\n",
        "      loss += y_train[i] * math.log(y_pred[i] + epsilon) + (1 - y_train[i]) * math.log(1 - y_pred[i] + epsilon)\n",
        "  # adding epsilon so not exactly 0 and 1\n",
        "\n",
        "  return -loss / m   # mean of loss\n",
        "\n",
        "# calculate gradient to use to update the model parameters\n",
        "@njit\n",
        "def gradient(y_train, y_pred, X_train):\n",
        "  m = len(y_train)\n",
        "  difference = y_pred -  y_train.reshape(-1)\n",
        "  weight_update = np.dot(X_train.T, difference) / m\n",
        "  bias_update = np.sum(difference) / m\n",
        "  return weight_update, bias_update\n",
        "\n",
        "@njit(parallel=True, fastmath = True)\n",
        "def update_parameters(weights, bias, weight_update, bias_update, learning_rate = 0.1):\n",
        "  for i in prange(weights.shape[0]):\n",
        "    weights[i] -= learning_rate * weight_update[i]\n",
        "  bias -= learning_rate * bias_update\n",
        "  return weights, bias\n",
        "\n",
        "# calculating weights and sigmoid calculation\n",
        "def fit(X_train, y_train, epochs=200, learning_rate = 0.1):\n",
        "  # given values\n",
        "  x_rows, x_columns = X_train.shape # grabbing the columns this will be # of weights\n",
        "  weights = np.random.rand(x_columns) * 0.01  # initializing weights to small random values\n",
        "  bias = 0  # initializing bias to zero\n",
        "  y_train = y_train.reshape(-1, 1)  # Ensuring that y_train is of shape (m, 1)\n",
        "\n",
        "  epoch_times = [] # Time for each epochs run\n",
        "\n",
        "  # for a certain amount of epochs apply gradient\n",
        "  for epoch in range(epochs):\n",
        "    start_time = time.time() # let's start the epochs going\n",
        "\n",
        "    # applying logistic regression getting a y_pred\n",
        "    y_pred = calculating_logistic_regression(X_train, weights, bias)\n",
        "\n",
        "    # after we will apply the gradient to find out how to update weights and bias\n",
        "    weight_update, bias_update = gradient(y_train, y_pred, X_train)\n",
        "    weights, bias = update_parameters(weights, bias, weight_update, bias_update, learning_rate)\n",
        "\n",
        "    # calculating loss\n",
        "    loss = compute_loss(y_train, y_pred)\n",
        "\n",
        "    # time ended\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "    epoch_times.append(epoch_time)\n",
        "\n",
        "    print(\"Epoch: {} Loss: {} Time: {:.4f} seconds\".format(epoch, loss, epoch_time))\n",
        "\n",
        "  avg_epoch_time = np.mean(epoch_times[1:]) # except first cause getting set up\n",
        "  print(\"Average time per epoch: {:.4f} seconds\".format(avg_epoch_time))\n",
        "\n",
        "  return weights, bias\n",
        "\n",
        "@njit\n",
        "def predict(X_test, weights, bias):\n",
        "  return calculating_logistic_regression(X_test, weights, bias)\n",
        "\n",
        "# load the breast cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# split the train and test dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "\n",
        "# now my part comes in will replace the previous LogisticRegression with my own\n",
        "weights, bias = fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = predict(X_test, weights, bias)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression model accuracy (in %):\", acc * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgeFD0kLVdwI",
        "outputId": "6c25ec8e-808e-4cef-8d46-9290a77e6952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 7.87939554988054 Time: 3.8976 seconds\n",
            "Epoch: 1 Loss: 12.843870254606342 Time: 0.8492 seconds\n",
            "Epoch: 2 Loss: 12.843870254606351 Time: 0.0002 seconds\n",
            "Epoch: 3 Loss: 7.879395549880542 Time: 0.0007 seconds\n",
            "Epoch: 4 Loss: 12.843870254606351 Time: 0.0007 seconds\n",
            "Epoch: 5 Loss: 7.879395549880542 Time: 0.0006 seconds\n",
            "Epoch: 6 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 7 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 8 Loss: 7.879395549880542 Time: 0.0006 seconds\n",
            "Epoch: 9 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 10 Loss: 7.879395549880542 Time: 0.0006 seconds\n",
            "Epoch: 11 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 12 Loss: 11.24977288227478 Time: 0.0006 seconds\n",
            "Epoch: 13 Loss: 7.879395549880542 Time: 0.0008 seconds\n",
            "Epoch: 14 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 15 Loss: 7.879395549880542 Time: 0.0006 seconds\n",
            "Epoch: 16 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 17 Loss: 2.231736311462807 Time: 0.0006 seconds\n",
            "Epoch: 18 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 19 Loss: 7.87939554988054 Time: 0.0006 seconds\n",
            "Epoch: 20 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 21 Loss: 7.8338499108581106 Time: 0.0001 seconds\n",
            "Epoch: 22 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 23 Loss: 9.519038592512219 Time: 0.0006 seconds\n",
            "Epoch: 24 Loss: 7.879395549880542 Time: 0.0006 seconds\n",
            "Epoch: 25 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 26 Loss: 7.651667354768387 Time: 0.0006 seconds\n",
            "Epoch: 27 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 28 Loss: 5.192202866105511 Time: 0.0006 seconds\n",
            "Epoch: 29 Loss: 7.469484798678662 Time: 0.0006 seconds\n",
            "Epoch: 30 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 31 Loss: 2.732738347983425 Time: 0.0006 seconds\n",
            "Epoch: 32 Loss: 2.049553756100471 Time: 0.0001 seconds\n",
            "Epoch: 33 Loss: 12.70723333699351 Time: 0.0006 seconds\n",
            "Epoch: 34 Loss: 7.879395549880542 Time: 0.0006 seconds\n",
            "Epoch: 35 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 36 Loss: 7.742758632813249 Time: 0.0006 seconds\n",
            "Epoch: 37 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 38 Loss: 2.1861906738951515 Time: 0.0007 seconds\n",
            "Epoch: 39 Loss: 12.024048748747504 Time: 0.0006 seconds\n",
            "Epoch: 40 Loss: 7.879395549880542 Time: 0.0006 seconds\n",
            "Epoch: 41 Loss: 12.843870254606342 Time: 0.0007 seconds\n",
            "Epoch: 42 Loss: 7.515030437701094 Time: 0.0007 seconds\n",
            "Epoch: 43 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 44 Loss: 2.5050101455973923 Time: 0.0001 seconds\n",
            "Epoch: 45 Loss: 12.798324615402073 Time: 0.0006 seconds\n",
            "Epoch: 46 Loss: 7.879395549880542 Time: 0.0010 seconds\n",
            "Epoch: 47 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 48 Loss: 6.968482769431924 Time: 0.0006 seconds\n",
            "Epoch: 49 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 50 Loss: 3.643651120976317 Time: 0.0006 seconds\n",
            "Epoch: 51 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 52 Loss: 7.8338499108581106 Time: 0.0006 seconds\n",
            "Epoch: 53 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 54 Loss: 4.9189290134225345 Time: 0.0007 seconds\n",
            "Epoch: 55 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 56 Loss: 7.33284788161137 Time: 0.0006 seconds\n",
            "Epoch: 57 Loss: 12.843870254606351 Time: 0.0001 seconds\n",
            "Epoch: 58 Loss: 3.962470594133333 Time: 0.0006 seconds\n",
            "Epoch: 59 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 60 Loss: 7.697212993790818 Time: 0.0006 seconds\n",
            "Epoch: 61 Loss: 12.843870254606342 Time: 0.0006 seconds\n",
            "Epoch: 62 Loss: 4.964474652444965 Time: 0.0001 seconds\n",
            "Epoch: 63 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 64 Loss: 7.196210964544078 Time: 0.0006 seconds\n",
            "Epoch: 65 Loss: 12.843870254606351 Time: 0.0007 seconds\n",
            "Epoch: 66 Loss: 4.5090182624025035 Time: 0.0006 seconds\n",
            "Epoch: 67 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 68 Loss: 7.423939159656232 Time: 0.0006 seconds\n",
            "Epoch: 69 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 70 Loss: 4.827837735377671 Time: 0.0006 seconds\n",
            "Epoch: 71 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 72 Loss: 7.196210964544078 Time: 0.0006 seconds\n",
            "Epoch: 73 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 74 Loss: 4.69120081831038 Time: 0.0006 seconds\n",
            "Epoch: 75 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 76 Loss: 7.196210964544078 Time: 0.0006 seconds\n",
            "Epoch: 77 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 78 Loss: 4.9189290134225345 Time: 0.0006 seconds\n",
            "Epoch: 79 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 80 Loss: 7.105119686499216 Time: 0.0006 seconds\n",
            "Epoch: 81 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 82 Loss: 4.9189290134225345 Time: 0.0001 seconds\n",
            "Epoch: 83 Loss: 12.798324615402073 Time: 0.0006 seconds\n",
            "Epoch: 84 Loss: 7.105119686499216 Time: 0.0006 seconds\n",
            "Epoch: 85 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 86 Loss: 4.9189290134225345 Time: 0.0006 seconds\n",
            "Epoch: 87 Loss: 12.798324615402073 Time: 0.0006 seconds\n",
            "Epoch: 88 Loss: 7.059574047476785 Time: 0.0006 seconds\n",
            "Epoch: 89 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 90 Loss: 4.9189290134225345 Time: 0.0005 seconds\n",
            "Epoch: 91 Loss: 12.707233336993518 Time: 0.0006 seconds\n",
            "Epoch: 92 Loss: 7.0140284084543545 Time: 0.0006 seconds\n",
            "Epoch: 93 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 94 Loss: 5.283294125601982 Time: 0.0006 seconds\n",
            "Epoch: 95 Loss: 12.707233336993518 Time: 0.0006 seconds\n",
            "Epoch: 96 Loss: 6.695208935297337 Time: 0.0006 seconds\n",
            "Epoch: 97 Loss: 12.843870254606351 Time: 0.0006 seconds\n",
            "Epoch: 98 Loss: 5.556567959736566 Time: 0.0006 seconds\n",
            "Epoch: 99 Loss: 12.707233336993518 Time: 0.0006 seconds\n",
            "Epoch: 100 Loss: 6.376389462140322 Time: 0.0006 seconds\n",
            "Epoch: 101 Loss: 12.798324615402073 Time: 0.0006 seconds\n",
            "Epoch: 102 Loss: 5.966478710938444 Time: 0.0006 seconds\n",
            "Epoch: 103 Loss: 12.798324615402073 Time: 0.0006 seconds\n",
            "Epoch: 104 Loss: 5.966478710938444 Time: 0.0006 seconds\n",
            "Epoch: 105 Loss: 12.707233336993518 Time: 0.0006 seconds\n",
            "Epoch: 106 Loss: 6.103115628005736 Time: 0.0006 seconds\n",
            "Epoch: 107 Loss: 12.798324615402064 Time: 0.0006 seconds\n",
            "Epoch: 108 Loss: 5.966478710938444 Time: 0.0006 seconds\n",
            "Epoch: 109 Loss: 12.707233336993518 Time: 0.0006 seconds\n",
            "Epoch: 110 Loss: 5.966478710938444 Time: 0.0006 seconds\n",
            "Epoch: 111 Loss: 12.661687697789239 Time: 0.0006 seconds\n",
            "Epoch: 112 Loss: 5.966478710938444 Time: 0.0006 seconds\n",
            "Epoch: 113 Loss: 12.661687697789239 Time: 0.0006 seconds\n",
            "Epoch: 114 Loss: 5.966478710938444 Time: 0.0006 seconds\n",
            "Epoch: 115 Loss: 12.570596419380685 Time: 0.0005 seconds\n",
            "Epoch: 116 Loss: 5.966478710938444 Time: 0.0006 seconds\n",
            "Epoch: 117 Loss: 12.47950514097213 Time: 0.0006 seconds\n",
            "Epoch: 118 Loss: 5.966478710938444 Time: 0.0006 seconds\n",
            "Epoch: 119 Loss: 12.43395950176785 Time: 0.0006 seconds\n",
            "Epoch: 120 Loss: 5.966478710938444 Time: 0.0006 seconds\n",
            "Epoch: 121 Loss: 12.43395950176785 Time: 0.0006 seconds\n",
            "Epoch: 122 Loss: 5.829841793871152 Time: 0.0006 seconds\n",
            "Epoch: 123 Loss: 12.115140027337905 Time: 0.0006 seconds\n",
            "Epoch: 124 Loss: 5.966478710938444 Time: 0.0006 seconds\n",
            "Epoch: 125 Loss: 12.206231305746462 Time: 0.0006 seconds\n",
            "Epoch: 126 Loss: 5.693204876803859 Time: 0.0006 seconds\n",
            "Epoch: 127 Loss: 11.887411831134669 Time: 0.0006 seconds\n",
            "Epoch: 128 Loss: 5.966478710938444 Time: 0.0006 seconds\n",
            "Epoch: 129 Loss: 11.932957470520794 Time: 0.0006 seconds\n",
            "Epoch: 130 Loss: 5.556567959736566 Time: 0.0006 seconds\n",
            "Epoch: 131 Loss: 11.386409799887609 Time: 0.0006 seconds\n",
            "Epoch: 132 Loss: 5.829841793871152 Time: 0.0006 seconds\n",
            "Epoch: 133 Loss: 11.887411831134669 Time: 0.0006 seconds\n",
            "Epoch: 134 Loss: 5.693204876803859 Time: 0.0006 seconds\n",
            "Epoch: 135 Loss: 11.386409799887609 Time: 0.0006 seconds\n",
            "Epoch: 136 Loss: 5.602113598758996 Time: 0.0006 seconds\n",
            "Epoch: 137 Loss: 11.158681603866224 Time: 0.0006 seconds\n",
            "Epoch: 138 Loss: 5.647659237781427 Time: 0.0006 seconds\n",
            "Epoch: 139 Loss: 11.022044686253391 Time: 0.0006 seconds\n",
            "Epoch: 140 Loss: 5.511022320714134 Time: 0.0006 seconds\n",
            "Epoch: 141 Loss: 10.657679572619168 Time: 0.0006 seconds\n",
            "Epoch: 142 Loss: 5.465476681691704 Time: 0.0007 seconds\n",
            "Epoch: 143 Loss: 10.33886009818922 Time: 0.0004 seconds\n",
            "Epoch: 144 Loss: 5.192202847557119 Time: 0.0006 seconds\n",
            "Epoch: 145 Loss: 9.33685603569511 Time: 0.0006 seconds\n",
            "Epoch: 146 Loss: 4.827837735377671 Time: 0.0006 seconds\n",
            "Epoch: 147 Loss: 8.562580169222388 Time: 0.0006 seconds\n",
            "Epoch: 148 Loss: 4.3723813451533635 Time: 0.0006 seconds\n",
            "Epoch: 149 Loss: 6.92293715750469 Time: 0.0006 seconds\n",
            "Epoch: 150 Loss: 3.6891967599987474 Time: 0.0006 seconds\n",
            "Epoch: 151 Loss: 5.920933094465037 Time: 0.0006 seconds\n",
            "Epoch: 152 Loss: 3.0971034527071466 Time: 0.0006 seconds\n",
            "Epoch: 153 Loss: 4.918929031425384 Time: 0.0006 seconds\n",
            "Epoch: 154 Loss: 2.3228275895076687 Time: 0.0006 seconds\n",
            "Epoch: 155 Loss: 3.9169249682038845 Time: 0.0006 seconds\n",
            "Epoch: 156 Loss: 2.049553756646012 Time: 0.0006 seconds\n",
            "Epoch: 157 Loss: 3.461468575979258 Time: 0.0006 seconds\n",
            "Epoch: 158 Loss: 2.0040081179872753 Time: 0.0006 seconds\n",
            "Epoch: 159 Loss: 3.14264910118562 Time: 0.0006 seconds\n",
            "Epoch: 160 Loss: 1.958462479692232 Time: 0.0006 seconds\n",
            "Epoch: 161 Loss: 2.823829626573828 Time: 0.0006 seconds\n",
            "Epoch: 162 Loss: 2.1406450366911907 Time: 0.0006 seconds\n",
            "Epoch: 163 Loss: 2.5050101521438832 Time: 0.0006 seconds\n",
            "Epoch: 164 Loss: 2.0950993980324535 Time: 0.0006 seconds\n",
            "Epoch: 165 Loss: 2.7782839873695493 Time: 0.0007 seconds\n",
            "Epoch: 166 Loss: 2.1406450366911907 Time: 0.0006 seconds\n",
            "Epoch: 167 Loss: 2.5050101521438832 Time: 0.0006 seconds\n",
            "Epoch: 168 Loss: 2.0950993980324535 Time: 0.0007 seconds\n",
            "Epoch: 169 Loss: 2.823829626573828 Time: 0.0002 seconds\n",
            "Epoch: 170 Loss: 2.049553758282635 Time: 0.0006 seconds\n",
            "Epoch: 171 Loss: 2.823829626573828 Time: 0.0007 seconds\n",
            "Epoch: 172 Loss: 2.049553758282635 Time: 0.0006 seconds\n",
            "Epoch: 173 Loss: 2.823829626573828 Time: 0.0006 seconds\n",
            "Epoch: 174 Loss: 2.0950993974869125 Time: 0.0001 seconds\n",
            "Epoch: 175 Loss: 2.732738348165272 Time: 0.0006 seconds\n",
            "Epoch: 176 Loss: 2.1406450366911907 Time: 0.0002 seconds\n",
            "Epoch: 177 Loss: 2.732738348165272 Time: 0.0006 seconds\n",
            "Epoch: 178 Loss: 2.1406450366911907 Time: 0.0006 seconds\n",
            "Epoch: 179 Loss: 2.732738348165272 Time: 0.0001 seconds\n",
            "Epoch: 180 Loss: 2.1406450366911907 Time: 0.0001 seconds\n",
            "Epoch: 181 Loss: 2.732738348165272 Time: 0.0001 seconds\n",
            "Epoch: 182 Loss: 2.1406450366911907 Time: 0.0017 seconds\n",
            "Epoch: 183 Loss: 2.732738348165272 Time: 0.0007 seconds\n",
            "Epoch: 184 Loss: 2.0950993974869125 Time: 0.0006 seconds\n",
            "Epoch: 185 Loss: 2.914920904982383 Time: 0.0006 seconds\n",
            "Epoch: 186 Loss: 2.049553758100788 Time: 0.0006 seconds\n",
            "Epoch: 187 Loss: 2.914920904982383 Time: 0.0006 seconds\n",
            "Epoch: 188 Loss: 2.049553758100788 Time: 0.0006 seconds\n",
            "Epoch: 189 Loss: 2.914920904982383 Time: 0.0006 seconds\n",
            "Epoch: 190 Loss: 2.049553758100788 Time: 0.0006 seconds\n",
            "Epoch: 191 Loss: 2.960466544186661 Time: 0.0006 seconds\n",
            "Epoch: 192 Loss: 2.0040081188965098 Time: 0.0006 seconds\n",
            "Epoch: 193 Loss: 2.960466544186661 Time: 0.0006 seconds\n",
            "Epoch: 194 Loss: 2.0040081188965098 Time: 0.0006 seconds\n",
            "Epoch: 195 Loss: 2.960466544186661 Time: 0.0001 seconds\n",
            "Epoch: 196 Loss: 2.0040081188965098 Time: 0.0010 seconds\n",
            "Epoch: 197 Loss: 2.960466544186661 Time: 0.0006 seconds\n",
            "Epoch: 198 Loss: 2.0040081188965098 Time: 0.0006 seconds\n",
            "Epoch: 199 Loss: 2.960466544186661 Time: 0.0006 seconds\n",
            "Average time per epoch: 0.0048 seconds\n",
            "Logistic Regression model accuracy (in %): 92.10526315789474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation\n",
        "- Process: I first fit my x_train data by running logistical regression on it and adjusting the weights and bias for each epochs accordingly (gradient) to better adjust the y_pred with the y_train. I then applied the same logistical regression steps with the weights and bias discovered from before to the x_test and compared the y_pred_test to the y_test results.\n",
        "(This was sooo cool to open up and ACTUALLY understand AI functions that I had seen before).\n",
        "- For my parallelization, my non-parallelized version had an average time per epoch of 0.0066 seconds, and my parallelized version had an average time per epoch of 0.0048 seconds.\n",
        "- Interestingly enough, I at first saw an increase in time (increase of ~0.02 seconds) every time I added a new @njit(parallel=True) to replace a numpy function where there was not heavy complex operations. The overhead of using numba was seen. I think the reason for this is that Numba is lot more beneficial when you are doing really complex heavy operations in for loops instead of replacing Numpy's vectorized functions already. I saw a huge time decrease when I switched out my old code for computing loss to my new code (went from 0.0215 to 0.0048 seconds)\n",
        "\n",
        "```\n",
        "# old code\n",
        "epsilon = 1e-9\n",
        "  loss = y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon) # added epsilon so not exactly 0 and 1\n",
        "  return -np.mean(loss) can we make this into a for loop\n",
        "```\n",
        "\n",
        "```\n",
        "# new code\n",
        "@njit(parallel=True, fastmath = True)\n",
        "def compute_loss(y_train, y_pred):\n",
        "  epsilon = 1e-9\n",
        "  m = len(y_train)\n",
        "  loss = 0.0\n",
        "  y_train = y_train.reshape(-1) # fixing up data\n",
        "  y_pred = y_pred.reshape(-1)\n",
        "    \n",
        "  for i in prange(m):\n",
        "      loss += y_train[i] * math.log(y_pred[i] + epsilon) + (1 - y_train[i]) * math.log(1 - y_pred[i] + epsilon)\n",
        "\n",
        "  return -loss / m\n",
        "```\n",
        "\n",
        "- Using Numba arguments, I played around with different parameters and saw an increase when I used \"fastmath = True\". This was specifically great for the floating-point optimizations seen in compute_loss.\n",
        "- Also, in my Numba code, I use prange with the parallel=True. This creates threads to process the for loop functions. But, since a lot of the functions for my logistic functions did not involve a for loop there were not a lot of possibilities/ places for Numba functions.  \n",
        "- One last thing I noticed was an overflow that would occur \"RuntimeWarning: overflow encountered in exp return 1 / (1 + np.exp(-x)).\" I learned, after I looked into the sigmoid function more, that it is a result from x being a very large negative value and e^-x exceeding the max floating-point number. I was considering clipping the values (creating bounds), but I was not sure how low I should set the bar. So I left it as is for now.\n",
        "\n",
        "\n",
        "Results\n",
        "- Before parallelization: average time per epoch = 0.0066 seconds, model accuracy = 85.09%\n",
        "- After parallelization: average time per epoch = 0.0048 seconds, model accuracy = 92.11%"
      ],
      "metadata": {
        "id": "DNlmTfFEhBQW"
      }
    }
  ]
}